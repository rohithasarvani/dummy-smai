{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b6ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"ReLU Activation\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.out = np.maximum(0, x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (self.out > 0)\n",
    "        return grad_input\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"Tanh Activation\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.out = np.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * (1 - self.out**2)\n",
    "        return grad_input\n",
    "\n",
    "class Sigmoid:\n",
    "    \"\"\"Sigmoid Activation\"\"\"\n",
    "    def forward(self, x):\n",
    "        self.out = 1 / (1 + np.exp(-x))\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        grad_input = grad_output * self.out * (1 - self.out)\n",
    "        return grad_input\n",
    "\n",
    "class Identity:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output\n",
    "\n",
    "    def update(self, lr):\n",
    "        pass\n",
    "\n",
    "    def zero_grad(self):\n",
    "        pass  # nothing to reset, but must exist for consistency\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Identity()\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2707f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \"\"\"Fully Connected Layer\"\"\"\n",
    "    def __init__(self, in_features, out_features, activation):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.activation = activation\n",
    "\n",
    "        # Initialize weights and biases\n",
    "        self.W = np.random.randn(in_features, out_features) * np.sqrt(2.0 / in_features)\n",
    "        self.b = np.zeros((1, out_features))\n",
    "\n",
    "        # Cumulative gradients\n",
    "        self.dW_cum = np.zeros_like(self.W)\n",
    "        self.db_cum = np.zeros_like(self.b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.input = x  # Save for backward\n",
    "        self.linear_out = x @ self.W + self.b\n",
    "        self.out = self.activation.forward(self.linear_out)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        # Gradient w.r.t activation\n",
    "        grad_activation = self.activation.backward(grad_output)\n",
    "        # Gradients w.r.t weights and biases\n",
    "        self.dW_cum += self.input.T @ grad_activation\n",
    "        self.db_cum += np.sum(grad_activation, axis=0, keepdims=True)\n",
    "        # Gradient w.r.t input for previous layer\n",
    "        grad_input = grad_activation @ self.W.T\n",
    "        return grad_input\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.dW_cum.fill(0)\n",
    "        self.db_cum.fill(0)\n",
    "\n",
    "    def update(self, lr=0.01):\n",
    "        self.W -= lr * self.dW_cum\n",
    "        self.b -= lr * self.db_cum\n",
    "        self.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fa9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \"\"\"Neural Network Model\"\"\"\n",
    "    def __init__(self, layers, loss_type=\"MSE\"):\n",
    "        self.layers = layers\n",
    "        self.loss_type = loss_type\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def train(self, x, y):\n",
    "        \"\"\"Forward + backward pass, returns scalar loss\"\"\"\n",
    "        y_pred = self.forward(x)\n",
    "        # Compute loss\n",
    "        if self.loss_type == \"MSE\":\n",
    "            loss = np.mean((y_pred - y) ** 2)\n",
    "            grad_loss = 2 * (y_pred - y) / y.shape[0]\n",
    "        elif self.loss_type == \"BCE\":\n",
    "            eps = 1e-9\n",
    "            y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "            loss = -np.mean(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "            grad_loss = (y_pred - y) / (y_pred * (1 - y_pred)) / y.shape[0]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown loss type\")\n",
    "\n",
    "        self.backward(grad_loss)\n",
    "        return float(loss)\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zero_grad()\n",
    "\n",
    "    def update(self, lr=0.01):\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr=lr)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def save_to(self, path):\n",
    "        data = {}\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            # only save layers that have weights\n",
    "            if hasattr(layer, \"W\") and hasattr(layer, \"b\"):\n",
    "                data[f\"W_{idx}\"] = layer.W\n",
    "                data[f\"b_{idx}\"] = layer.b\n",
    "        np.savez(path, **data)\n",
    "\n",
    "    def load_from(self, path):\n",
    "        loaded = np.load(path)\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            w_key = f\"W_{idx}\"\n",
    "            b_key = f\"b_{idx}\"\n",
    "            if w_key not in loaded or b_key not in loaded:\n",
    "                raise ValueError(\"Architecture mismatch!\")\n",
    "            if layer.W.shape != loaded[w_key].shape or layer.b.shape != loaded[b_key].shape:\n",
    "                raise ValueError(\"Shape mismatch!\")\n",
    "            layer.W = loaded[w_key]\n",
    "            layer.b = loaded[b_key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded1251c",
   "metadata": {},
   "source": [
    "# 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae260c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Import Libraries and Load MNIST Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Loading MNIST dataset...\")\n",
    "# Load MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, parser='auto')\n",
    "X = mnist.data.values if hasattr(mnist.data, 'values') else mnist.data\n",
    "y = mnist.target.values if hasattr(mnist.target, 'values') else mnist.target\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "X = X.astype(np.float32) / 255.0\n",
    "\n",
    "# Convert labels to integers\n",
    "y = y.astype(int)\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total samples: {X.shape[0]}\")\n",
    "print(f\"Feature dimension: {X.shape[1]}\")\n",
    "print(f\"Image shape: 28x28\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "\n",
    "# Split into train and test sets (MNIST already has standard split)\n",
    "# First 60000 samples are training, rest are test\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7e5006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Visualize Sample MNIST Images\n",
    "def visualize_mnist_samples(X, y, n_samples=10, title=\"MNIST Sample Images\"):\n",
    "    \"\"\"\n",
    "    Visualize random samples from MNIST dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : np.ndarray\n",
    "        Image data (N, 784)\n",
    "    y : np.ndarray\n",
    "        Labels (N,)\n",
    "    n_samples : int\n",
    "        Number of samples to display\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Randomly select samples\n",
    "    indices = np.random.choice(len(X), n_samples, replace=False)\n",
    "    \n",
    "    for idx, ax in enumerate(axes):\n",
    "        img = X[indices[idx]].reshape(28, 28)\n",
    "        ax.imshow(img, cmap='gray')\n",
    "        ax.set_title(f'Label: {y[indices[idx]]}', fontsize=11)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.suptitle(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize training samples\n",
    "visualize_mnist_samples(X_train, y_train, n_samples=10, \n",
    "                        title=\"Random Training Samples from MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486a598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - MLPAutoencoder Class Implementation\n",
    "class MLPAutoencoder:\n",
    "    \"\"\"\n",
    "    Multi-Layer Perceptron Autoencoder for image reconstruction.\n",
    "    \n",
    "    Uses encoder to compress input to latent representation,\n",
    "    and decoder to reconstruct input from latent representation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=784, hidden_dims=[256, 128], latent_dim=64):\n",
    "        \"\"\"\n",
    "        Initialize MLPAutoencoder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        input_dim : int\n",
    "            Dimension of input (e.g., 784 for 28x28 images)\n",
    "        hidden_dims : list of int\n",
    "            Hidden layer dimensions for encoder\n",
    "        latent_dim : int\n",
    "            Dimension of latent bottleneck representation\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build encoder layers\n",
    "        self.encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        # Hidden layers in encoder\n",
    "        for hidden_dim in hidden_dims:\n",
    "            self.encoder_layers.append(Linear(prev_dim, hidden_dim, ReLU()))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Bottleneck layer (encoder output)\n",
    "        self.encoder_layers.append(Linear(prev_dim, latent_dim, ReLU()))\n",
    "        \n",
    "        # Build decoder layers (mirror of encoder)\n",
    "        self.decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        \n",
    "        # Hidden layers in decoder (reverse order)\n",
    "        for hidden_dim in reversed(hidden_dims):\n",
    "            self.decoder_layers.append(Linear(prev_dim, hidden_dim, ReLU()))\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (decoder output) - use Sigmoid to constrain to [0, 1]\n",
    "        self.decoder_layers.append(Linear(prev_dim, input_dim, Sigmoid()))\n",
    "        \n",
    "        # Combine all layers\n",
    "        self.all_layers = self.encoder_layers + self.decoder_layers\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent representation.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.ndarray\n",
    "            Input data (N, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Latent representation (N, latent_dim)\n",
    "        \"\"\"\n",
    "        out = x\n",
    "        for layer in self.encoder_layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent representation to reconstructed input.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        z : np.ndarray\n",
    "            Latent representation (N, latent_dim)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Reconstructed input (N, input_dim)\n",
    "        \"\"\"\n",
    "        out = z\n",
    "        for layer in self.decoder_layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass: encode then decode.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.ndarray\n",
    "            Input data (N, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray\n",
    "            Reconstructed input (N, input_dim)\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        latent = self.encode(x)\n",
    "        # Decode\n",
    "        reconstructed = self.decode(latent)\n",
    "        return reconstructed\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Backward pass through entire autoencoder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        grad_output : np.ndarray\n",
    "            Gradient of loss w.r.t. output\n",
    "        \"\"\"\n",
    "        grad = grad_output\n",
    "        for layer in reversed(self.all_layers):\n",
    "            grad = layer.backward(grad)\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Reset all gradients to zero.\"\"\"\n",
    "        for layer in self.all_layers:\n",
    "            layer.zero_grad()\n",
    "    \n",
    "    def update(self, lr=0.01):\n",
    "        \"\"\"\n",
    "        Update all parameters using accumulated gradients.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lr : float\n",
    "            Learning rate\n",
    "        \"\"\"\n",
    "        for layer in self.all_layers:\n",
    "            layer.update(lr=lr)\n",
    "    \n",
    "    def train_step(self, x):\n",
    "        \"\"\"\n",
    "        Single training step: forward, compute loss, backward.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x : np.ndarray\n",
    "            Input batch (N, input_dim)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        float\n",
    "            Reconstruction loss (MSE)\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        x_reconstructed = self.forward(x)\n",
    "        \n",
    "        # Compute MSE loss\n",
    "        loss = np.mean((x_reconstructed - x) ** 2)\n",
    "        \n",
    "        # Compute gradient of loss w.r.t. output\n",
    "        grad_loss = 2 * (x_reconstructed - x) / x.shape[0]\n",
    "        \n",
    "        # Backward pass\n",
    "        self.backward(grad_loss)\n",
    "        \n",
    "        return float(loss)\n",
    "    \n",
    "    def get_architecture_summary(self):\n",
    "        \"\"\"Return string summary of autoencoder architecture.\"\"\"\n",
    "        summary = \"=\" * 70 + \"\\n\"\n",
    "        summary += \"MLPAutoencoder Architecture\\n\"\n",
    "        summary += \"=\" * 70 + \"\\n\"\n",
    "        summary += f\"Input Dimension: {self.input_dim}\\n\"\n",
    "        summary += f\"Latent Dimension: {self.latent_dim}\\n\"\n",
    "        summary += f\"Hidden Dimensions: {self.hidden_dims}\\n\\n\"\n",
    "        \n",
    "        summary += \"Encoder:\\n\"\n",
    "        summary += \"-\" * 70 + \"\\n\"\n",
    "        prev_dim = self.input_dim\n",
    "        for i, dim in enumerate(self.hidden_dims):\n",
    "            summary += f\"  Layer {i+1}: Linear({prev_dim} ‚Üí {dim}) + ReLU\\n\"\n",
    "            prev_dim = dim\n",
    "        summary += f\"  Bottleneck: Linear({prev_dim} ‚Üí {self.latent_dim}) + ReLU\\n\\n\"\n",
    "        \n",
    "        summary += \"Decoder:\\n\"\n",
    "        summary += \"-\" * 70 + \"\\n\"\n",
    "        prev_dim = self.latent_dim\n",
    "        for i, dim in enumerate(reversed(self.hidden_dims)):\n",
    "            summary += f\"  Layer {i+1}: Linear({prev_dim} ‚Üí {dim}) + ReLU\\n\"\n",
    "            prev_dim = dim\n",
    "        summary += f\"  Output: Linear({prev_dim} ‚Üí {self.input_dim}) + Sigmoid\\n\"\n",
    "        \n",
    "        summary += \"=\" * 70\n",
    "        return summary\n",
    "\n",
    "# Create autoencoder instance\n",
    "autoencoder = MLPAutoencoder(\n",
    "    input_dim=784,\n",
    "    hidden_dims=[256, 128],\n",
    "    latent_dim=64\n",
    ")\n",
    "\n",
    "# Print architecture summary\n",
    "print(autoencoder.get_architecture_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae97467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Training Function with Visualization\n",
    "def train_autoencoder(autoencoder, X_train, X_test, batch_size=128, num_epochs=20, \n",
    "                      lr=0.001, patience=5, rel_loss_thresh=0.01):\n",
    "    \"\"\"\n",
    "    Train the autoencoder on MNIST dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    autoencoder : MLPAutoencoder\n",
    "        The autoencoder model to train\n",
    "    X_train : np.ndarray\n",
    "        Training data (N, 784)\n",
    "    X_test : np.ndarray\n",
    "        Test data (M, 784)\n",
    "    batch_size : int\n",
    "        Batch size for training\n",
    "    num_epochs : int\n",
    "        Maximum number of epochs\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    patience : int\n",
    "        Early stopping patience\n",
    "    rel_loss_thresh : float\n",
    "        Relative improvement threshold for early stopping\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing training history and metadata\n",
    "    \"\"\"\n",
    "    num_samples = X_train.shape[0]\n",
    "    train_loss_history = []\n",
    "    test_loss_history = []\n",
    "    epoch_list = []\n",
    "    \n",
    "    best_loss = np.inf\n",
    "    epochs_no_improve = 0\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"TRAINING AUTOENCODER\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Training samples: {num_samples}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    print(f\"Max epochs: {num_epochs}\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle training data\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X_shuffled = X_train[indices]\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        num_batches = int(np.ceil(num_samples / batch_size))\n",
    "        \n",
    "        # Training loop\n",
    "        for i in range(0, num_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            \n",
    "            # Zero gradients\n",
    "            autoencoder.zero_grad()\n",
    "            \n",
    "            # Forward pass and compute loss\n",
    "            batch_loss = autoencoder.train_step(X_batch)\n",
    "            \n",
    "            # Update parameters\n",
    "            autoencoder.update(lr=lr)\n",
    "            \n",
    "            epoch_loss += batch_loss\n",
    "        \n",
    "        # Average training loss for epoch\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "        train_loss_history.append(avg_train_loss)\n",
    "        \n",
    "        # Compute test loss\n",
    "        test_loss = 0.0\n",
    "        num_test_batches = int(np.ceil(X_test.shape[0] / batch_size))\n",
    "        for i in range(0, X_test.shape[0], batch_size):\n",
    "            X_test_batch = X_test[i:i+batch_size]\n",
    "            X_test_reconstructed = autoencoder.forward(X_test_batch)\n",
    "            test_loss += np.mean((X_test_reconstructed - X_test_batch) ** 2)\n",
    "        \n",
    "        avg_test_loss = test_loss / num_test_batches\n",
    "        test_loss_history.append(avg_test_loss)\n",
    "        \n",
    "        epoch_list.append(epoch + 1)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f} - \"\n",
    "              f\"Test Loss: {avg_test_loss:.6f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_train_loss < best_loss * (1 - rel_loss_thresh):\n",
    "            best_loss = avg_train_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"TRAINING COMPLETED\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Final Train Loss: {train_loss_history[-1]:.6f}\")\n",
    "    print(f\"Final Test Loss: {test_loss_history[-1]:.6f}\")\n",
    "    print(f\"Best Train Loss: {min(train_loss_history):.6f}\")\n",
    "    print(f\"Best Test Loss: {min(test_loss_history):.6f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'train_loss': train_loss_history,\n",
    "        'test_loss': test_loss_history,\n",
    "        'epochs': epoch_list,\n",
    "        'best_train_loss': min(train_loss_history),\n",
    "        'best_test_loss': min(test_loss_history)\n",
    "    }\n",
    "\n",
    "# Train the autoencoder\n",
    "training_history = train_autoencoder(\n",
    "    autoencoder=autoencoder,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    batch_size=128,\n",
    "    num_epochs=20,\n",
    "    lr=0.001,\n",
    "    patience=5,\n",
    "    rel_loss_thresh=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fb46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 - Plot Training History\n",
    "def plot_training_history(history, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot training and test loss over epochs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    history : dict\n",
    "        Dictionary containing training history\n",
    "    save_path : str, optional\n",
    "        Path to save the plot\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    epochs = history['epochs']\n",
    "    train_loss = history['train_loss']\n",
    "    test_loss = history['test_loss']\n",
    "    \n",
    "    # Linear scale plot\n",
    "    axes[0].plot(epochs, train_loss, 'o-', linewidth=2, markersize=6, \n",
    "                label='Training Loss', color='#FF6B6B')\n",
    "    axes[0].plot(epochs, test_loss, 's-', linewidth=2, markersize=6, \n",
    "                label='Test Loss', color='#4ECDC4')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[0].set_ylabel('Reconstruction Loss (MSE)', fontsize=12)\n",
    "    axes[0].set_title('Training History - Linear Scale', fontsize=13, fontweight='bold')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Log scale plot\n",
    "    axes[1].plot(epochs, train_loss, 'o-', linewidth=2, markersize=6, \n",
    "                label='Training Loss', color='#FF6B6B')\n",
    "    axes[1].plot(epochs, test_loss, 's-', linewidth=2, markersize=6, \n",
    "                label='Test Loss', color='#4ECDC4')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Reconstruction Loss (MSE) - Log Scale', fontsize=12)\n",
    "    axes[1].set_title('Training History - Log Scale', fontsize=13, fontweight='bold')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3, which='both')\n",
    "    axes[1].set_yscale('log')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a85b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Alternative Compact Visualization (One Sample Per Digit)\n",
    "def visualize_single_reconstruction_per_digit(autoencoder, X_test, y_test, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualize one original and reconstructed image for each digit class (0-9).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    autoencoder : MLPAutoencoder\n",
    "        Trained autoencoder model\n",
    "    X_test : np.ndarray\n",
    "        Test images (N, 784)\n",
    "    y_test : np.ndarray\n",
    "        Test labels (N,)\n",
    "    save_path : str, optional\n",
    "        Path to save the visualization\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 5))\n",
    "    \n",
    "    for digit in range(10):\n",
    "        # Get first occurrence of this digit\n",
    "        digit_idx = np.where(y_test == digit)[0][0]\n",
    "        \n",
    "        # Get original image\n",
    "        original = X_test[digit_idx:digit_idx+1]\n",
    "        \n",
    "        # Get reconstruction\n",
    "        reconstructed = autoencoder.forward(original)\n",
    "        \n",
    "        # Reshape for display\n",
    "        original_img = original.reshape(28, 28)\n",
    "        reconstructed_img = reconstructed.reshape(28, 28)\n",
    "        \n",
    "        # Plot original (top row)\n",
    "        axes[0, digit].imshow(original_img, cmap='gray')\n",
    "        axes[0, digit].set_title(f'Digit {digit}', fontsize=11, fontweight='bold')\n",
    "        axes[0, digit].axis('off')\n",
    "        \n",
    "        # Plot reconstruction (bottom row)\n",
    "        axes[1, digit].imshow(reconstructed_img, cmap='gray')\n",
    "        axes[1, digit].axis('off')\n",
    "    \n",
    "    # Add row labels\n",
    "    axes[0, 0].text(-0.5, 0.5, 'Original', fontsize=13, fontweight='bold',\n",
    "                   rotation=90, transform=axes[0, 0].transAxes,\n",
    "                   verticalalignment='center')\n",
    "    axes[1, 0].text(-0.5, 0.5, 'Reconstructed', fontsize=13, fontweight='bold',\n",
    "                   rotation=90, transform=axes[1, 0].transAxes,\n",
    "                   verticalalignment='center')\n",
    "    \n",
    "    plt.suptitle('Autoencoder Performance: One Sample Per Digit Class', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Visualize one sample per digit\n",
    "visualize_single_reconstruction_per_digit(\n",
    "    autoencoder=autoencoder,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa76f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 - Compute and Visualize Reconstruction Error by Digit\n",
    "def compute_reconstruction_error_by_digit(autoencoder, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Compute average reconstruction error for each digit class.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    autoencoder : MLPAutoencoder\n",
    "        Trained autoencoder model\n",
    "    X_test : np.ndarray\n",
    "        Test images (N, 784)\n",
    "    y_test : np.ndarray\n",
    "        Test labels (N,)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary mapping digit to average reconstruction error\n",
    "    \"\"\"\n",
    "    reconstruction_errors = {}\n",
    "    \n",
    "    for digit in range(10):\n",
    "        # Get all samples for this digit\n",
    "        digit_indices = np.where(y_test == digit)[0]\n",
    "        X_digit = X_test[digit_indices]\n",
    "        \n",
    "        # Get reconstructions\n",
    "        X_reconstructed = autoencoder.forward(X_digit)\n",
    "        \n",
    "        # Compute MSE for each sample\n",
    "        mse_per_sample = np.mean((X_reconstructed - X_digit) ** 2, axis=1)\n",
    "        \n",
    "        # Average over all samples of this digit\n",
    "        avg_error = np.mean(mse_per_sample)\n",
    "        reconstruction_errors[digit] = avg_error\n",
    "    \n",
    "    return reconstruction_errors\n",
    "\n",
    "def plot_reconstruction_error_by_digit(reconstruction_errors, save_path=None):\n",
    "    \"\"\"\n",
    "    Plot average reconstruction error for each digit class.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    reconstruction_errors : dict\n",
    "        Dictionary mapping digit to reconstruction error\n",
    "    save_path : str, optional\n",
    "        Path to save the plot\n",
    "    \"\"\"\n",
    "    digits = list(reconstruction_errors.keys())\n",
    "    errors = list(reconstruction_errors.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    bars = plt.bar(digits, errors, color='#4ECDC4', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # Color the bar with highest error differently\n",
    "    max_error_digit = max(reconstruction_errors, key=reconstruction_errors.get)\n",
    "    bars[max_error_digit].set_color('#FF6B6B')\n",
    "    \n",
    "    plt.xlabel('Digit Class', fontsize=12)\n",
    "    plt.ylabel('Average Reconstruction Error (MSE)', fontsize=12)\n",
    "    plt.title('Reconstruction Error by Digit Class', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(digits)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (digit, error) in enumerate(reconstruction_errors.items()):\n",
    "        plt.text(digit, error + 0.0001, f'{error:.5f}', \n",
    "                ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Compute reconstruction errors\n",
    "reconstruction_errors = compute_reconstruction_error_by_digit(autoencoder, X_test, y_test)\n",
    "\n",
    "# Print errors\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECONSTRUCTION ERROR BY DIGIT CLASS\")\n",
    "print(\"=\"*60)\n",
    "for digit in range(10):\n",
    "    print(f\"Digit {digit}: {reconstruction_errors[digit]:.6f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Plot errors\n",
    "plot_reconstruction_error_by_digit(reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63501c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ==========================================================\n",
    "# # ‚úÖ Cell 13 - Save Model and Results (Safe version)\n",
    "# # ==========================================================\n",
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# def save_autoencoder_results(autoencoder, training_history=None, reconstruction_errors=None, \n",
    "#                              latent_stats=None, save_dir=\"autoencoder_results\"):\n",
    "#     \"\"\"\n",
    "#     Save all autoencoder results to disk.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     autoencoder : MLPAutoencoder\n",
    "#         Trained autoencoder model\n",
    "#     training_history : dict or None\n",
    "#         Training history (optional)\n",
    "#     reconstruction_errors : dict or None\n",
    "#         Reconstruction errors by class (optional)\n",
    "#     latent_stats : dict or None\n",
    "#         Latent space statistics (optional)\n",
    "#     save_dir : str\n",
    "#         Directory to save results\n",
    "#     \"\"\"\n",
    "#     os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "#     # ------------------------------------------------------\n",
    "#     # ‚úÖ 1. Save model weights\n",
    "#     # ------------------------------------------------------\n",
    "#     model_data = {}\n",
    "#     for idx, layer in enumerate(autoencoder.all_layers):\n",
    "#         if hasattr(layer, \"W\") and hasattr(layer, \"b\"):\n",
    "#             model_data[f\"W_{idx}\"] = layer.W\n",
    "#             model_data[f\"b_{idx}\"] = layer.b\n",
    "\n",
    "#     model_path = os.path.join(save_dir, \"autoencoder_weights.npz\")\n",
    "#     np.savez(model_path, **model_data)\n",
    "#     print(f\"‚úÖ Model weights saved to: {model_path}\")\n",
    "    \n",
    "#     # ------------------------------------------------------\n",
    "#     # ‚úÖ 2. Save training history (if available)\n",
    "#     # ------------------------------------------------------\n",
    "#     if training_history is not None:\n",
    "#         history_path = os.path.join(save_dir, \"training_history.npz\")\n",
    "#         np.savez(history_path, \n",
    "#                  epochs=training_history.get('epochs', np.arange(len(training_history.get('train_loss', [])))),\n",
    "#                  train_loss=training_history.get('train_loss', []),\n",
    "#                  test_loss=training_history.get('test_loss', []))\n",
    "#         print(f\"‚úÖ Training history saved to: {history_path}\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Skipping: training_history not provided.\")\n",
    "    \n",
    "#     # ------------------------------------------------------\n",
    "#     # ‚úÖ 3. Save reconstruction errors (if available)\n",
    "#     # ------------------------------------------------------\n",
    "#     if reconstruction_errors is not None:\n",
    "#         errors_path = os.path.join(save_dir, \"reconstruction_errors.npz\")\n",
    "#         np.savez(errors_path, **{f\"class_{k}\": v for k, v in reconstruction_errors.items()})\n",
    "#         print(f\"‚úÖ Reconstruction errors saved to: {errors_path}\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Skipping: reconstruction_errors not provided.\")\n",
    "    \n",
    "#     # ------------------------------------------------------\n",
    "#     # ‚úÖ 4. Save latent statistics (optional)\n",
    "#     # ------------------------------------------------------\n",
    "#     if latent_stats is not None:\n",
    "#         latent_path = os.path.join(save_dir, \"latent_stats.npz\")\n",
    "#         np.savez(latent_path, **latent_stats)\n",
    "#         print(f\"‚úÖ Latent stats saved to: {latent_path}\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Skipping: latent_stats not provided.\")\n",
    "    \n",
    "#     # ------------------------------------------------------\n",
    "#     # ‚úÖ 5. Save architecture info\n",
    "#     # ------------------------------------------------------\n",
    "#     arch_path = os.path.join(save_dir, \"architecture.txt\")\n",
    "#     if hasattr(autoencoder, \"get_architecture_summary\"):\n",
    "#         with open(arch_path, 'w') as f:\n",
    "#             f.write(autoencoder.get_architecture_summary())\n",
    "#         print(f\"‚úÖ Architecture summary saved to: {arch_path}\")\n",
    "#     else:\n",
    "#         print(\"‚ö†Ô∏è Autoencoder missing get_architecture_summary() method.\")\n",
    "    \n",
    "#     print(f\"\\nüìÅ All results saved to directory: {save_dir}\")\n",
    "\n",
    "\n",
    "# # ==========================================================\n",
    "# # ‚úÖ Dummy defaults if not defined earlier\n",
    "# # ==========================================================\n",
    "# if 'training_history' not in locals():\n",
    "#     training_history = {\"epochs\": np.arange(10), \"train_loss\": np.random.rand(10), \"test_loss\": np.random.rand(10)}\n",
    "\n",
    "# if 'reconstruction_errors' not in locals():\n",
    "#     reconstruction_errors = {\"0\": np.random.rand(10), \"1\": np.random.rand(10)}\n",
    "\n",
    "# if 'latent_stats' not in locals():\n",
    "#     latent_stats = {\"mean\": np.random.rand(5), \"std\": np.random.rand(5)}\n",
    "\n",
    "# # ==========================================================\n",
    "# # ‚úÖ Call the function safely\n",
    "# # ==========================================================\n",
    "# save_autoencoder_results(\n",
    "#     autoencoder=autoencoder,\n",
    "#     training_history=training_history,\n",
    "#     reconstruction_errors=reconstruction_errors,\n",
    "#     latent_stats=latent_stats,\n",
    "#     save_dir=\"autoencoder_results\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d314b46b",
   "metadata": {},
   "source": [
    "# 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d39399",
   "metadata": {},
   "source": [
    "# 3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3456642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Load LFW Dataset and Prepare Data\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Path to LFW dataset\n",
    "lfw_path = \"/home/rohitha/ass3/LFW_Dataset\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING LFW DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dictionary to store images by person\n",
    "person_images = defaultdict(list)\n",
    "\n",
    "# Load all images\n",
    "for person_name in os.listdir(lfw_path):\n",
    "    person_dir = os.path.join(lfw_path, person_name)\n",
    "    \n",
    "    if not os.path.isdir(person_dir):\n",
    "        continue\n",
    "    \n",
    "    for img_file in os.listdir(person_dir):\n",
    "        if img_file.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            img_path = os.path.join(person_dir, img_file)\n",
    "            try:\n",
    "                # Load image and convert to grayscale\n",
    "                img = Image.open(img_path).convert('L')\n",
    "                img_array = np.array(img, dtype=np.float32)\n",
    "                person_images[person_name].append(img_array)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {img_path}: {e}\")\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"\\nTotal people in dataset: {len(person_images)}\")\n",
    "for person, images in sorted(person_images.items(), key=lambda x: len(x[1]), reverse=True)[:10]:\n",
    "    print(f\"  {person}: {len(images)} images\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a0d051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Prepare Train/Test Splits for Anomaly Detection\n",
    "\n",
    "# Extract George W Bush images (normal class)\n",
    "normal_class = 'George_W_Bush'\n",
    "normal_images = person_images[normal_class]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARING TRAIN/TEST SPLITS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nNormal class: {normal_class}\")\n",
    "print(f\"Total normal images: {len(normal_images)}\")\n",
    "\n",
    "# Use 400 images for training, 100 for testing (from George W Bush)\n",
    "np.random.shuffle(normal_images)\n",
    "normal_train = normal_images[:400]\n",
    "normal_test = normal_images[400:500]\n",
    "\n",
    "print(f\"Normal training images: {len(normal_train)}\")\n",
    "print(f\"Normal test images: {len(normal_test)}\")\n",
    "\n",
    "# Collect anomaly images from other classes\n",
    "anomaly_images = []\n",
    "anomaly_count_per_class = {}\n",
    "\n",
    "for person, images in person_images.items():\n",
    "    if person == normal_class:\n",
    "        continue\n",
    "    \n",
    "    # Take up to 10 images from each other person as anomalies\n",
    "    n_images = min(10, len(images))\n",
    "    anomaly_images.extend(images[:n_images])\n",
    "    anomaly_count_per_class[person] = n_images\n",
    "\n",
    "print(f\"\\nTotal anomaly images collected: {len(anomaly_images)}\")\n",
    "print(f\"Number of anomaly classes: {len(anomaly_count_per_class)}\")\n",
    "\n",
    "# Create test set: mix of normal and anomaly\n",
    "# Use 100 normal + anomalies\n",
    "test_images = normal_test + anomaly_images\n",
    "test_labels = [0] * len(normal_test) + [1] * len(anomaly_images)  # 0=normal, 1=anomaly\n",
    "\n",
    "# Shuffle test set\n",
    "test_indices = np.random.permutation(len(test_images))\n",
    "test_images = [test_images[i] for i in test_indices]\n",
    "test_labels = [test_labels[i] for i in test_indices]\n",
    "\n",
    "print(f\"\\nTest set composition:\")\n",
    "print(f\"  Normal images: {sum(1 for l in test_labels if l == 0)}\")\n",
    "print(f\"  Anomaly images: {sum(1 for l in test_labels if l == 1)}\")\n",
    "print(f\"  Total test images: {len(test_images)}\")\n",
    "\n",
    "# Get image dimensions\n",
    "img_height, img_width = normal_images[0].shape\n",
    "input_dim = img_height * img_width\n",
    "\n",
    "print(f\"\\nImage dimensions: {img_height}x{img_width}\")\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a96713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 - Preprocess and Normalize Data\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPROCESSING DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Convert training images to flattened arrays and normalize\n",
    "X_train_lfw = np.array([img.flatten() / 255.0 for img in normal_train], dtype=np.float32)\n",
    "\n",
    "# Convert test images to flattened arrays and normalize\n",
    "X_test_lfw = np.array([img.flatten() / 255.0 for img in test_images], dtype=np.float32)\n",
    "y_test_lfw = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "print(f\"Training data shape: {X_train_lfw.shape}\")\n",
    "print(f\"Test data shape: {X_test_lfw.shape}\")\n",
    "print(f\"Test labels shape: {y_test_lfw.shape}\")\n",
    "print(f\"\\nData type: {X_train_lfw.dtype}\")\n",
    "print(f\"Value range: [{X_train_lfw.min():.3f}, {X_train_lfw.max():.3f}]\")\n",
    "\n",
    "# Memory usage estimation\n",
    "train_memory = X_train_lfw.nbytes / (1024**2)  # MB\n",
    "test_memory = X_test_lfw.nbytes / (1024**2)  # MB\n",
    "print(f\"\\nMemory usage:\")\n",
    "print(f\"  Training data: {train_memory:.2f} MB\")\n",
    "print(f\"  Test data: {test_memory:.2f} MB\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db8951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 - Create Autoencoder for LFW Data\n",
    "\n",
    "# Define autoencoder architecture optimized for 250x250 grayscale images\n",
    "lfw_autoencoder = MLPAutoencoder(\n",
    "    input_dim=input_dim,  # 62500 for 250x250 images\n",
    "    hidden_dims=[2048, 512],  # Reduced complexity for faster training\n",
    "    latent_dim=128\n",
    ")\n",
    "\n",
    "print(lfw_autoencoder.get_architecture_summary())\n",
    "\n",
    "# Count total parameters\n",
    "total_params = 0\n",
    "for layer in lfw_autoencoder.all_layers:\n",
    "    if hasattr(layer, 'W'):\n",
    "        total_params += layer.W.size + layer.b.size\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f762c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train Loss: 0.333222 - Test Loss: 0.324495\n"
     ]
    }
   ],
   "source": [
    "# Cell 5 - Train Autoencoder on Normal Data (George W Bush)\n",
    "\n",
    "# Optimized training parameters for faster convergence\n",
    "lfw_training_history = train_autoencoder(\n",
    "    autoencoder=lfw_autoencoder,\n",
    "    X_train=X_train_lfw,\n",
    "    X_test=X_train_lfw[:50],  # Use small subset for validation during training\n",
    "    batch_size=8,  # Smaller batch size for memory efficiency\n",
    "    num_epochs=100,  # Reduced epochs\n",
    "    lr=0.01,  # Lower learning rate for stability\n",
    "    patience=15,\n",
    "    rel_loss_thresh=0.0001\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6d565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - Visualize Training Progress\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(lfw_training_history['epochs'], lfw_training_history['train_loss'], \n",
    "         label='Train Loss', marker='o', linewidth=2)\n",
    "plt.plot(lfw_training_history['epochs'], lfw_training_history['test_loss'], \n",
    "         label='Validation Loss', marker='s', linewidth=2)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Loss (MSE)', fontsize=12)\n",
    "plt.title('LFW Autoencoder Training Progress', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Training Loss: {lfw_training_history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Best Training Loss: {lfw_training_history['best_train_loss']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a035a424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 - Compute Reconstruction Errors on Test Set\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPUTING RECONSTRUCTION ERRORS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compute reconstruction errors for all test images\n",
    "reconstruction_errors = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(0, len(X_test_lfw), batch_size):\n",
    "    X_batch = X_test_lfw[i:i+batch_size]\n",
    "    X_reconstructed = lfw_autoencoder.forward(X_batch)\n",
    "    \n",
    "    # Compute MSE for each image in batch\n",
    "    batch_errors = np.mean((X_reconstructed - X_batch) ** 2, axis=1)\n",
    "    reconstruction_errors.extend(batch_errors)\n",
    "\n",
    "reconstruction_errors = np.array(reconstruction_errors)\n",
    "\n",
    "print(f\"Computed reconstruction errors for {len(reconstruction_errors)} test images\")\n",
    "print(f\"\\nReconstruction Error Statistics:\")\n",
    "print(f\"  Mean: {reconstruction_errors.mean():.6f}\")\n",
    "print(f\"  Std: {reconstruction_errors.std():.6f}\")\n",
    "print(f\"  Min: {reconstruction_errors.min():.6f}\")\n",
    "print(f\"  Max: {reconstruction_errors.max():.6f}\")\n",
    "print(f\"  Median: {np.median(reconstruction_errors):.6f}\")\n",
    "\n",
    "# Separate errors by class\n",
    "normal_errors = reconstruction_errors[y_test_lfw == 0]\n",
    "anomaly_errors = reconstruction_errors[y_test_lfw == 1]\n",
    "\n",
    "print(f\"\\nNormal Images (n={len(normal_errors)}):\")\n",
    "print(f\"  Mean Error: {normal_errors.mean():.6f}\")\n",
    "print(f\"  Std Error: {normal_errors.std():.6f}\")\n",
    "\n",
    "print(f\"\\nAnomaly Images (n={len(anomaly_errors)}):\")\n",
    "print(f\"  Mean Error: {anomaly_errors.mean():.6f}\")\n",
    "print(f\"  Std Error: {anomaly_errors.std():.6f}\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1befbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 - Determine Optimal Threshold\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DETERMINING OPTIMAL THRESHOLD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Strategy: Use percentile of normal reconstruction errors\n",
    "# Common approaches: mean + k*std, or percentile-based\n",
    "\n",
    "# Method 1: Mean + k*std of normal errors\n",
    "threshold_1 = normal_errors.mean() + 2 * normal_errors.std()\n",
    "\n",
    "# Method 2: 95th percentile of normal errors\n",
    "threshold_2 = np.percentile(normal_errors, 95)\n",
    "\n",
    "# Method 3: Mean of normal errors\n",
    "threshold_3 = normal_errors.mean()\n",
    "\n",
    "print(f\"\\nCandidate Thresholds:\")\n",
    "print(f\"  Method 1 (Mean + 2*Std): {threshold_1:.6f}\")\n",
    "print(f\"  Method 2 (95th percentile): {threshold_2:.6f}\")\n",
    "print(f\"  Method 3 (Mean): {threshold_3:.6f}\")\n",
    "\n",
    "# Choose threshold that best separates normal and anomaly\n",
    "# We'll use Method 2 (95th percentile) as it's commonly used\n",
    "optimal_threshold = threshold_2\n",
    "\n",
    "print(f\"\\nSelected Threshold: {optimal_threshold:.6f}\")\n",
    "\n",
    "# Classification: error > threshold => anomaly\n",
    "y_pred = (reconstruction_errors > optimal_threshold).astype(int)\n",
    "\n",
    "print(f\"\\nPredictions:\")\n",
    "print(f\"  Classified as Normal: {np.sum(y_pred == 0)}\")\n",
    "print(f\"  Classified as Anomaly: {np.sum(y_pred == 1)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1289f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 - Calculate Evaluation Metrics\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_curve\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate metrics\n",
    "auc_score = roc_auc_score(y_test_lfw, reconstruction_errors)\n",
    "precision = precision_score(y_test_lfw, y_pred)\n",
    "recall = recall_score(y_test_lfw, y_pred)\n",
    "f1 = f1_score(y_test_lfw, y_pred)\n",
    "\n",
    "print(f\"\\nPerformance Metrics:\")\n",
    "print(f\"  AUC Score:  {auc_score:.4f}\")\n",
    "print(f\"  Precision:  {precision:.4f}\")\n",
    "print(f\"  Recall:     {recall:.4f}\")\n",
    "print(f\"  F1-Score:   {f1:.4f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test_lfw, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Negatives:  {tn}\")\n",
    "print(f\"  False Positives: {fp}\")\n",
    "print(f\"  False Negatives: {fn}\")\n",
    "print(f\"  True Positives:  {tp}\")\n",
    "\n",
    "print(f\"\\nClassification Breakdown:\")\n",
    "print(f\"  Accuracy: {(tn + tp) / len(y_test_lfw):.4f}\")\n",
    "print(f\"  Specificity: {tn / (tn + fp):.4f}\" if (tn + fp) > 0 else \"  Specificity: N/A\")\n",
    "print(f\"  Sensitivity (Recall): {recall:.4f}\")\n",
    "\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc432df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 - Visualize Sample Reconstructions\n",
    "\n",
    "# Select samples to visualize\n",
    "# 2 correctly classified normals, 2 correctly classified anomalies\n",
    "# 1 misclassified normal, 1 misclassified anomaly (if any)\n",
    "\n",
    "normal_indices = np.where(y_test_lfw == 0)[0]\n",
    "anomaly_indices = np.where(y_test_lfw == 1)[0]\n",
    "\n",
    "# Correctly classified\n",
    "correct_normal_idx = normal_indices[(y_pred[normal_indices] == 0)][:2]\n",
    "correct_anomaly_idx = anomaly_indices[(y_pred[anomaly_indices] == 1)][:2]\n",
    "\n",
    "# Misclassified (if any exist)\n",
    "misclass_normal_idx = normal_indices[(y_pred[normal_indices] == 1)][:1]\n",
    "misclass_anomaly_idx = anomaly_indices[(y_pred[anomaly_indices] == 0)][:1]\n",
    "\n",
    "# Combine all indices\n",
    "sample_indices = np.concatenate([\n",
    "    correct_normal_idx,\n",
    "    correct_anomaly_idx,\n",
    "    misclass_normal_idx,\n",
    "    misclass_anomaly_idx\n",
    "])\n",
    "\n",
    "# Generate reconstructions\n",
    "sample_original = X_test_lfw[sample_indices]\n",
    "sample_reconstructed = lfw_autoencoder.forward(sample_original)\n",
    "\n",
    "# Visualize\n",
    "n_samples = len(sample_indices)\n",
    "fig, axes = plt.subplots(2, n_samples, figsize=(3*n_samples, 6))\n",
    "\n",
    "titles = (\n",
    "    ['Correct Normal']*len(correct_normal_idx) +\n",
    "    ['Correct Anomaly']*len(correct_anomaly_idx) +\n",
    "    ['Misclass. Normal']*len(misclass_normal_idx) +\n",
    "    ['Misclass. Anomaly']*len(misclass_anomaly_idx)\n",
    ")\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Original\n",
    "    axes[0, i].imshow(sample_original[i].reshape(img_height, img_width), cmap='gray')\n",
    "    axes[0, i].set_title(f'{titles[i]}\\nError: {reconstruction_errors[idx]:.4f}', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Reconstructed\n",
    "    axes[1, i].imshow(sample_reconstructed[i].reshape(img_height, img_width), cmap='gray')\n",
    "    axes[1, i].set_title('Reconstructed', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Original', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Reconstructed', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Sample Reconstructions: Normal vs Anomaly', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e114be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
